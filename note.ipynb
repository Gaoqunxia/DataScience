
Jupyter NotebookLogout 拉勾网数据分析实例 Last Checkpoint: 2 hours ago (autosaved) Python 3
Python 3 Trusted
File
Edit
View
Insert
Cell
Kernel
Widgets
Help

In [60]:

1
import pandas as pd
2
import requests
3
import time
4
from collections import defaultdict
5
​
6
headers = {
7
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36",
8
    "Referer": "https://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?city=%E5%8C%97%E4%BA%AC&cl=false&fromSearch=true&labelWords=&suginput=",
9
}
10
​
11
cookies = {
12
    "Cookie": "JSESSIONID=ABAAABAAADEAAFI3E2AE80419BA8FFB6F5FFB49D1573C34; _ga=GA1.2.260226169.1531815206; user_trace_token=20180717161326-47e61066-8999-11e8-9c54-525400f775ce; LGUID=20180717161326-47e613e7-8999-11e8-9c54-525400f775ce; _gid=GA1.2.1025242393.1534486196; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1534486196; TG-TRACK-CODE=index_search; fromsite=translate.baiducontent.com; utm_source=""; index_location_city=%E5%8C%97%E4%BA%AC; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1534650094; _gat=1; LGSID=20180819114134-c4f1bc52-a361-11e8-a9f8-5254005c3644; PRE_UTM=; PRE_HOST=; PRE_SITE=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_%3FlabelWords%3D%26fromSearch%3Dtrue%26suginput%3D; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%3Fcity%3D%25E5%258C%2597%25E4%25BA%25AC%26cl%3Dfalse%26fromSearch%3Dtrue%26labelWords%3D%26suginput%3D; LGRID=20180819114134-c4f1bf1a-a361-11e8-a9f8-5254005c3644; SEARCH_ID=c5acefc2c73649dea43d17ec5f57b810"
13
}
14
​
15
​
16
# 爬取每一页的数据
17
def crawl_lagou_data(city, kd, pn=1):
18
    url = "https://www.lagou.com/jobs/positionAjax.json?city={city}&needAddtionalResult=false".format(city=city)
19
    
20
    data = {
21
    "first": True,
22
    "pn": pn,
23
    "kd": kd
24
    }
25
    rp = requests.post(url, headers=headers, cookies=cookies, data=data)
26
    
27
    result = rp.json()
28
    if not result["success"]:
29
        print("pn: %s, 抓取失败" % pn)
30
        return []
31
    return result["content"]["positionResult"]["result"]
32
​
33
result = defaultdict(list)
34
​
35
​
36
for i in range(1, 15):
37
    print("正在抓取第 %s 页" %i, end="\r")
38
    job_infos = crawl_lagou_data("广州", "数据分析", i)
39
    for job_info in job_infos:
40
        result["companyId"].append(job_info.get("companyId"))
41
        result["positionId"].append(job_info.get("positionId"))
42
        result["city"].append(job_info.get("city"))
43
        result["companyLabelList"].append(",".join(job_info.get("companyLabelList")))
44
        result["companyShortName"].append(job_info.get("companyShortName"))
45
        result["companySize"].append(job_info.get("companySize"))
46
        result["district"].append(job_info.get("district"))
47
        result["education"].append(job_info.get("education"))
48
        result["salary"].append(job_info.get("salary"))
49
        result["workYear"].append(job_info.get("workYear"))
50
        result["financeStage"].append(job_info.get("financeStage"))
51
        result["subwayline"].append(job_info.get("subwayline"))     
52
    print("抓取第 %s 页成功" %i, end="\n")#\r光标回退至开始位置
53
    if i==14:
54
        print("共完成%s页"%i)
55
    time.sleep(10)
56
df = pd.DataFrame(result)
抓取第 1 页成功
抓取第 2 页成功
抓取第 3 页成功
抓取第 4 页成功
抓取第 5 页成功
抓取第 6 页成功
抓取第 7 页成功
抓取第 8 页成功
抓取第 9 页成功
抓取第 10 页成功
抓取第 11 页成功
抓取第 12 页成功
抓取第 13 页成功
抓取第 14 页成功
共完成14页
In [61]:

1
print(df.tail())
    city  companyId      companyLabelList companyShortName companySize  \
205   广州     184659  上市公司,行业TOP,交通补助,带薪年假             蓝色光标     2000人以上   
206   广州       1311   带薪年假,定期体检,年度旅游,弹性工作               凡科    150-500人   
207   广州     227746   年底双薪,绩效奖金,带薪年假,定期体检               宝舟     50-150人   
208   广州       8414   带薪年假,午餐补助,扁平管理,弹性工作             一点资讯   500-2000人   
209   广州     342809   年底双薪,带薪年假,绩效奖金,年终分红              千千氏     50-150人   

    district education financeStage  positionId   salary subwayline workYear  
205      天河区        本科         上市公司     3406969  12k-18k       APM线     3-5年  
206      海珠区        本科         上市公司     5164338    6k-8k        8号线    应届毕业生  
207      天河区        本科        不需要融资     4956727   7k-14k       None     1-3年  
208      天河区        本科        D轮及以上     4086969   6k-12k       APM线     1-3年  
209      番禺区        大专          未融资     4272716   8k-13k       None     3-5年  
In [46]:

1
print("去重前数据：{}".format(df.shape))
2
df = df.drop_duplicates(subset=["positionId"])
3
print("去重后数据：{}".format(df.shape))
4
df.to_csv('df.csv', index=False)
去重前数据：(210, 12)
去重后数据：(210, 12)
In [47]:

1
import os
2
os.getcwd()
Out[47]:
'/Users/lin/数据分析'
In [48]:

1
%matplotlib inline
2
import pandas as pd
3
import matplotlib.pyplot as plt
4
plt.rcParams["font.sans-serif"]=["STHeiti"] #用来正常显示中文标签（mac版字体）
5
plt.rcParams["axes.unicode_minus"]=False #用来正常显示负号
In [49]:

1
plt.title("招聘数据分析岗位最多的公司Top10")
2
company_count = df.companyShortName.value_counts()[:10]
3
company_count.plot.bar(color='rgby');#颜色自定义

In [50]:

1
plt.title("招聘数据分析岗位对学历的要求")
2
df.education.value_counts().plot.pie(figsize=(5, 5), autopct="%.2f");

本科需求最大
In [51]:

1
plt.title("不同工作经验的数据分析岗位的市场需求")
2
df.workYear.value_counts().plot.pie(figsize=(5, 5), autopct="%.2f");

答案是3-5年的需求最大
In [52]:

1
plt.title("广州不同区的数据分析岗位机会对比")
2
data = df.district.value_counts().plot.barh(color='rgbcgk');

In [56]:

1
from pyecharts import Geo,Map
2
​
3
data = df.district.value_counts().to_dict()
4
print(data)
5
map1 = Map(
6
    "广州",
7
    "job",
8
    title_color="#fff",
9
    title_pos="center",
10
    width=600,
11
    height=400,
12
    background_color="#404a59",
13
)
14
attr, value = geo.cast(data)
15
​
16
map1.add(
17
    "",
18
    attr,
19
    value,
20
    visual_range=[0, 150],
21
    maptype="广州",
22
    visual_text_color="#fff",
23
    symbol_size=15,
24
    is_visualmap=True,
25
​
26
)
27
map1.render("广州.html")#显示在当前目录的html文件
{'天河区': 112, '越秀区': 28, '海珠区': 27, '番禺区': 25, '黄埔区': 6, '荔湾区': 5, '萝岗区': 4, '白云区': 2, '增城市': 1}
In [54]:

1
​
2
def calculate_salary_mean(salary_str):
3
    salary_sum = 0
4
    
5
    salary_list= salary_str.upper().replace("K", "000").split("-")
6
    
7
    for i in salary_list:
8
        salary_sum += int(i)
9
    
10
    return salary_sum / len(salary_list)
11
​
12
# 计算每条招聘信息中新增的平均薪资
13
df["salary_mean"] = df["salary"].map(lambda x: calculate_salary_mean(x))
14
​
15
plt.title("不同工作年限的数据分析岗位的薪资对比")
16
work_year_salary_mane = df.groupby("workYear")["salary_mean"].mean()
17
work_year_salary_mane.plot.bar(color='rgbyk');

